{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Example, Twitter Tweets <img style=\"float: right; width: 310px;\" src=\"./Data/Twitter_Logo.jpg\"/>  \n",
    "  \n",
    "---  \n",
    "\n",
    "### By: Heather M. Steich, M.S.\n",
    "### Date: October 29$^{th}$, 2017\n",
    "### Written in: Python 3.4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.5 |Anaconda custom (64-bit)| (default, Jul  5 2016, 14:53:07) [MSC v.1600 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "  \n",
    "## Dataset Credit  \n",
    "  \n",
    "  \n",
    "The data used for this project is used with permission (if cited) from the following source:  \n",
    "\n",
    "    Z. Cheng, J. Caverlee, and K. Lee. You Are Where You Tweet: A Content-Based Approach to Geo-locating Twitter Users. \n",
    "    In Proceeding of the 19th ACM Conference on Information and Knowledge Management (CIKM), Toronto, Oct 2010. (Bibtex)\n",
    "\n",
    "<https://archive.org/details/twitter_cikm_2010><img style=\"float: center;\" src=\"./Data/paper_logo.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "## Overview\n",
    "\n",
    "The goal of the exercise is to extract information about concert appearances of musicians, performers or bands.  For each such tweet, we are looking to extract:  \n",
    "\n",
    " - Who was the performer  \n",
    " - When was the show  \n",
    " - Where was the show  \n",
    " - The Tweeter user who attended it  \n",
    " - The sentiment of the tweet  \n",
    "   \n",
    "Not all of these fields are available in all tweets, and that’s ok.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the dataset includes the user id who sent the tweet and the timestamp for the tweet. For the ‘when’ field, we are interested in the date of the show (not just the tweet). We are not interested in any other tweets, including tweets about performers which don’t mention concerts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "  \n",
    "### Part 1: Classify if the tweets are relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## LOAD LIBRARIES\n",
    "\n",
    "# Data wrangling & processing: \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine learning:\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.ensemble import RandomForestClassifier as RF\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#from pandas_ml import ConfusionMatrix\n",
    "#from sklearn.metrics import roc_curve\n",
    "#from sklearn.ensemble import RandomForestRegressor as RFR\n",
    "\n",
    "# Plotting:\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from IPython.display import display, HTML\n",
    "\n",
    "# Remove warning messages:\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ESTABLISH PLOT FORMATTING\n",
    "\n",
    "#mpl.rcdefaults()  # Resets plot defaults\n",
    "\n",
    "def plt_format():\n",
    "    %matplotlib inline\n",
    "    plt.rcParams['figure.figsize'] = (16, 10)\n",
    "    plt.rcParams['font.size'] = 16\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    plt.rcParams['axes.labelcolor'] = 'black'\n",
    "    plt.rcParams['axes.labelsize'] = 20\n",
    "    plt.rcParams['axes.labelweight'] = 'bold'\n",
    "    plt.rcParams['axes.titlesize'] = 32\n",
    "    plt.rcParams['axes.titleweight'] = 'bold'\n",
    "    plt.rcParams['legend.fontsize'] = 16\n",
    "    plt.rcParams['legend.markerscale'] = 4\n",
    "    plt.rcParams['text.color'] = 'black'\n",
    "    plt.rcParams['xtick.labelsize'] = 20\n",
    "    plt.rcParams['ytick.labelsize'] = 20\n",
    "    plt.rcParams['legend.fontsize'] = 16\n",
    "    plt.rcParams['legend.frameon'] = False\n",
    "    plt.rcParams['axes.linewidth'] = 1\n",
    "\n",
    "#plt.rcParams.keys()  # Available rcParams\n",
    "plt_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Step 2: Load, view & prepare the provided data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (3741881, 4)\n",
      "Train Column Names: Index(['UserID', 'tTweetID', 'tTweet', 'tCreatedAt'], dtype='object')\n",
      "\n",
      "Test Shape: (5125748, 4)\n",
      "Test Column Names: Index(['UserID', 'tTweetID', 'tTweet', 'tCreatedAt'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## LOAD DATA:\n",
    "\n",
    "# Read in the files:\n",
    "train = pd.read_csv(\"./Data/corrected_training_set_tweets.csv\")\n",
    "test = pd.read_csv(\"./Data/corrected_test_set_tweets.csv\")\n",
    "\n",
    "# Translate the timestamps to DateTime objects:\n",
    "train.tCreatedAt = pd.to_datetime(train.tCreatedAt)\n",
    "test.tCreatedAt = pd.to_datetime(test.tCreatedAt)\n",
    "\n",
    "# Print shapes:\n",
    "print('Train Shape:', train.shape)\n",
    "print('Train Column Names:', train.columns)\n",
    "print('\\nTest Shape:', test.shape)\n",
    "print('Test Column Names:', test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>tTweetID</th>\n",
       "      <th>tTweet</th>\n",
       "      <th>tCreatedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60730027</td>\n",
       "      <td>6320951896</td>\n",
       "      <td>@thediscovietnam coo.  thanks. just dropped yo...</td>\n",
       "      <td>2009-12-03 18:41:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60730027</td>\n",
       "      <td>6320673258</td>\n",
       "      <td>@thediscovietnam shit it ain't lettin me DM yo...</td>\n",
       "      <td>2009-12-03 18:31:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60730027</td>\n",
       "      <td>6319871652</td>\n",
       "      <td>@thediscovietnam hey cody, quick question...ca...</td>\n",
       "      <td>2009-12-03 18:01:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60730027</td>\n",
       "      <td>6318151501</td>\n",
       "      <td>@smokinvinyl dang.  you need anything?  I got ...</td>\n",
       "      <td>2009-12-03 17:00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60730027</td>\n",
       "      <td>6317932721</td>\n",
       "      <td>maybe i'm late in the game on this one, but th...</td>\n",
       "      <td>2009-12-03 16:52:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     UserID    tTweetID                                             tTweet  \\\n",
       "0  60730027  6320951896  @thediscovietnam coo.  thanks. just dropped yo...   \n",
       "1  60730027  6320673258  @thediscovietnam shit it ain't lettin me DM yo...   \n",
       "2  60730027  6319871652  @thediscovietnam hey cody, quick question...ca...   \n",
       "3  60730027  6318151501  @smokinvinyl dang.  you need anything?  I got ...   \n",
       "4  60730027  6317932721  maybe i'm late in the game on this one, but th...   \n",
       "\n",
       "            tCreatedAt  \n",
       "0  2009-12-03 18:41:07  \n",
       "1  2009-12-03 18:31:01  \n",
       "2  2009-12-03 18:01:51  \n",
       "3  2009-12-03 17:00:16  \n",
       "4  2009-12-03 16:52:36  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## PRINT A PREVIEW OF THE DATAFRAMES:\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>tTweetID</th>\n",
       "      <th>tTweet</th>\n",
       "      <th>tCreatedAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22077441</td>\n",
       "      <td>10538487904</td>\n",
       "      <td>Ok today I have to find something to wear for ...</td>\n",
       "      <td>2010-03-15 17:35:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22077441</td>\n",
       "      <td>10536835844</td>\n",
       "      <td>I am glad I'm having this show but I can't wai...</td>\n",
       "      <td>2010-03-15 16:53:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22077441</td>\n",
       "      <td>10536809086</td>\n",
       "      <td>Honestly I don't even know what's going on any...</td>\n",
       "      <td>2010-03-15 16:52:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22077441</td>\n",
       "      <td>10534149786</td>\n",
       "      <td>@LovelyJ_Janelle hey sorry I'm sitting infront...</td>\n",
       "      <td>2010-03-15 15:42:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22077441</td>\n",
       "      <td>10530203659</td>\n",
       "      <td>Sitting infront of this sewing machine ... I d...</td>\n",
       "      <td>2010-03-15 13:55:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     UserID     tTweetID                                             tTweet  \\\n",
       "0  22077441  10538487904  Ok today I have to find something to wear for ...   \n",
       "1  22077441  10536835844  I am glad I'm having this show but I can't wai...   \n",
       "2  22077441  10536809086  Honestly I don't even know what's going on any...   \n",
       "3  22077441  10534149786  @LovelyJ_Janelle hey sorry I'm sitting infront...   \n",
       "4  22077441  10530203659  Sitting infront of this sewing machine ... I d...   \n",
       "\n",
       "            tCreatedAt  \n",
       "0  2010-03-15 17:35:58  \n",
       "1  2010-03-15 16:53:44  \n",
       "2  2010-03-15 16:52:59  \n",
       "3  2010-03-15 15:42:07  \n",
       "4  2010-03-15 13:55:22  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      " UserID                 int64\n",
      "tTweetID               int64\n",
      "tTweet                object\n",
      "tCreatedAt    datetime64[ns]\n",
      "dtype: object\n",
      "\n",
      "Testing:\n",
      " UserID                 int64\n",
      "tTweetID               int64\n",
      "tTweet                object\n",
      "tCreatedAt    datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "## CHECK DATA TYPES:\n",
    "\n",
    "print('Training:\\n', train.dtypes)\n",
    "print('\\nTesting:\\n', test.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The following blocks are from a different Notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Step 3: Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## BASIC STATISTICS FOR THE CLAIMS DATA:\n",
    "\n",
    "claim_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## THERE WAS ONE NEGATIVE CLAIM:\n",
    "\n",
    "claim_df[claim_df.ClaimedAmount < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## IT SEEMS LIKE THIS WAS A REVERSAL OF A CHARGE ON THE SAME DAY:\n",
    "\n",
    "claim_df[claim_df.PolicyId == 777949]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## BASIC STATISTICS FOR THE POLICY DATA:\n",
    "\n",
    "policy_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## VIEW POLICIES THAT DO NOT HAVE AN ASSIGNED MONTHLY PREMIUM:\n",
    "\n",
    "policy_df[policy_df.MonthlyPremium.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## CHECK THE DATE RANGES TO MAKE SURE THIS IS ALL 2016 DATA:\n",
    "\n",
    "print(\"Earliest Date of Claims\", claim_df.ClaimDate.min())\n",
    "print(\"Latest Date of Claims\", claim_df.ClaimDate.max())\n",
    "print(\"\\nEarliest Date of Policy Enrollment\", policy_df.EnrollDate.min())\n",
    "print(\"Latest Date of Policy Enrollment\", policy_df.EnrollDate.max())\n",
    "print(\"\\nEarliest Date of Policy Cancel\", policy_df.CancelDate.min())\n",
    "print(\"Latest Date of Policy Cancel\", policy_df.CancelDate.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## MANY CUSTOMERS ENROLLED LONG BEFORE 2016:  \n",
    "\n",
    "print(\"Number of Policy Holders Enrolled Prior to 2016:\", \n",
    "      policy_df[policy_df.EnrollDate < '2016'].shape[0])\n",
    "policy_df[policy_df.EnrollDate < '2016'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## COUNT OF UNIQUE POLICY ID'S:\n",
    "\n",
    "print(\"Unique Policy ID's:\", policy_df.PolicyId.unique().shape[0])\n",
    "print(\"Number of Duplicated Rows:\", sum(policy_df.duplicated()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## COUNT OF UNIQUE POLICY ID'S THAT FILED CLAIMS:\n",
    "\n",
    "print(\"Unique Policy ID's That Filed Claims:\", \n",
    "      claim_df.PolicyId.unique().shape[0])\n",
    "print(\"Number of Duplicated Rows:\", sum(claim_df.duplicated()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key finding:**  \n",
    "  \n",
    "**There are 3,744 claims that have duplicates; resulting in 6,616 rows (4.255%) are duplicates of another.**  \n",
    "  \n",
    "**This would be a pivotal time to reach out to check that these are in-fact valid claims.  Due to this being a homework assignment, I will make the executive decision that these *are* valid claims.  For example, perhaps the animal went to the veterinarian for two vaccines, which resulted in two identical claim amounts on the same day.**  \n",
    "\n",
    "**Please note that this is an assumption, and in the real world I would ask for further clarification prior to proceeding with analyses.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## MANY CLAIMS ARE DUPLICATED:\n",
    "\n",
    "print(\"Duplicated Claims Account for\", \n",
    "      claim_df[claim_df.duplicated(keep=False)].shape[0],\n",
    "      \"Rows of the Claims Data.  This is\",\n",
    "      round(claim_df[claim_df.duplicated(keep=False)].shape[0] \n",
    "      / claim_df.shape[0] * 100, 3), \"% of the Claims Data.\")\n",
    "claim_df[claim_df.duplicated(keep=False)].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Step 4: Transform & merge the data to a monthly-per-policy basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## ADD A LEVEL TO COLUMN INDEX TO ALIGN WITH THE MONTHLY CLAIMS DATA:\n",
    "\n",
    "# Re-assign DataFrame to another name:\n",
    "reindex_policy = pd.DataFrame(policy_df)\n",
    "\n",
    "# Reset the index level to prevent an error from occurring if \n",
    "# this cell is run more than once:\n",
    "try:\n",
    "    reindex_policy = reindex_policy[list(policy_df.columns.levels[0].values)]\n",
    "except: \n",
    "    reindex_policy = policy_df[list(policy_df.columns.values)]\n",
    "\n",
    "# Add extra level to column index:\n",
    "reindex_policy.columns = pd.MultiIndex.from_arrays([reindex_policy.columns, \n",
    "                    [' '] * len(reindex_policy.columns)])\n",
    "\n",
    "# Print preview of re-indexed policy DataFrame:\n",
    "reindex_policy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## PIVOT & MERGE THE TWO DATAFRAMES TO GET A MORE COMPLETE PICTURE OF \n",
    "## CLAIMS BEHAVIOUR, ADDING ADDITION METRICS (FEATURES) FOR MODELLING:\n",
    "\n",
    "# Pivot the claim DataFrame to have one row per policy, & a column\n",
    "# for every month's ClaimedAmount & PaidAmount; if not claim data, \n",
    "# fill as '$0' claim:\n",
    "pivot_df = pd.pivot_table(claim_df,index=claim_df['PolicyId'],\n",
    "               columns=claim_df['ClaimDate'].dt.month,\n",
    "               aggfunc=np.sum, fill_value=0).reset_index()\n",
    "\n",
    "# Merge the Claims and Policy DataFrames together in an 'left' join on PolicyId:\n",
    "wide_df = reindex_policy.join(pivot_df.set_index('PolicyId'))\n",
    "\n",
    "# Fill in missing amounts with zeros:\n",
    "wide_df['ClaimedAmount'] = wide_df.ClaimedAmount.fillna(0)\n",
    "wide_df['PaidAmount'] = wide_df.PaidAmount.fillna(0)\n",
    "\n",
    "# Calculate basic metrics for each policy:\n",
    "wide_df['MeanClaims'] = np.mean(wide_df.ClaimedAmount, axis=1)\n",
    "wide_df['MeanPaid'] = np.mean(wide_df.PaidAmount, axis=1)\n",
    "wide_df['MedianClaims'] = np.median(wide_df.ClaimedAmount, axis=1)\n",
    "wide_df['MedianPaid'] = np.median(wide_df.PaidAmount, axis=1)\n",
    "wide_df['TotalClaims'] = np.sum(wide_df.ClaimedAmount, axis=1)\n",
    "wide_df['TotalPaid'] = np.sum(wide_df.PaidAmount, axis=1)\n",
    "wide_df['TotalDifference'] =  (np.sum(wide_df.ClaimedAmount, axis=1) - \n",
    "                               np.sum(wide_df.PaidAmount, axis=1))\n",
    "wide_df['ProportionCovered'] =  (np.sum(wide_df.PaidAmount, axis=1) / \n",
    "                                    np.sum(wide_df.ClaimedAmount, axis=1))\n",
    "\n",
    "# Since we only have claims for 2016, calculate the number of months &  \n",
    "# paid premiums in 2016 alone:\n",
    "wide_df['Premiums2016'] = (np.where(pd.to_datetime(pd.Series(wide_df['CancelDate'])).dt.year == 2016,\n",
    "         pd.to_datetime(pd.Series(wide_df['CancelDate'])).dt.month, 12) \n",
    "         - np.where(pd.to_datetime(pd.Series(wide_df['EnrollDate'])).dt.year == 2016, \n",
    "         pd.to_datetime(pd.Series(wide_df['EnrollDate'])).dt.month, 0))\n",
    "wide_df['PremiumsPaid2016'] = np.multiply(wide_df.Premiums2016.values, \n",
    "                                       wide_df.MonthlyPremium.iloc[0:, 0].values)\n",
    "wide_df['PremiumVPaid'] = (wide_df.PremiumsPaid2016.values - wide_df.TotalPaid.values)\n",
    "\n",
    "# Use a binary key to mark if the customer is current or canceled at the end of 2016:\n",
    "wide_df['Churned'] = np.where(wide_df.CancelDate.fillna(0) \n",
    "                                       > pd.datetime(2016, 1, 1), 1, 0)\n",
    "\n",
    "# Calculate the total number of months the customer has held a policy:\n",
    "end = pd.to_datetime('2016-12-31')\n",
    "wide_df['PolicyLength'] = (wide_df.CancelDate.fillna(end) - \n",
    "                           wide_df.EnrollDate).astype('timedelta64[M]')\n",
    "\n",
    "# Print a preview of the prepared DataFrame:\n",
    "wide_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## WRITE OUT THE PREPARED DATA FRAME FOR FUTURE REFERENCE:\n",
    "\n",
    "# Condense the columns' MultiIndex for clarification prior to writing out:\n",
    "write_out = wide_df.copy(deep=True)\n",
    "write_out.columns =  [''.join(tuple(map(str, t))) for t in write_out.columns.values]\n",
    "write_out.to_csv('./Data/PreparedData.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key finding:**  \n",
    "  \n",
    "**Based on the provided data, using 2016 premiums collected (assuming *no* pro-rating for partial months) & paid claims only, Trupanion made a gross profit of $24,185,900.12!**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## DATA FACT == IN THE BLACK FOR 2016:\n",
    "\n",
    "# Calculation of the 2016 total sum of premiums paid to Trupanion minus paid claims:\n",
    "print('$', round(np.sum(wide_df.PremiumVPaid), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key finding:**  \n",
    "  \n",
    "**There are 5 policies missing data on monthly premiums.  I typically would ask for clarification here, but since there are so few I'll make the executive decision.  I believe that these rows should be dropped for a model on paid claims, because none of them had any claims submitted or paid.  However, since 4 out of the 5 rows were canceled policies, I would choose to keep them for a model on cancel predictions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## LOCATE POLICIES MISSING MONTHLY PREMIUM DATA & CREATE NEW DATAFRAMES FOR USE \n",
    "## IN PAID CLAIMS & CANCELATION MODELING:\n",
    "\n",
    "# Identify the rows with missing data:\n",
    "missing = np.argwhere(np.isnan(wide_df.xs('PremiumVPaid', axis=1, drop_level=True))).ravel()\n",
    "no_premium = wide_df[wide_df.index.isin(missing)] \n",
    "\n",
    "# DataFrame without the 5 rows:\n",
    "paid_df = wide_df[~wide_df.index.isin(no_premium.index)]\n",
    "\n",
    "# DataFrame with 5 missing premiums filled with zeros:\n",
    "no_premium[['MonthlyPremium', 'PremiumsPaid2016', 'PremiumVPaid']] = (\n",
    "    no_premium[['MonthlyPremium', 'PremiumsPaid2016', 'PremiumVPaid']].fillna(0))\n",
    "cancel_df = pd.concat([paid_df, no_premium])\n",
    "\n",
    "# Print the policies with missing premiums:\n",
    "no_premium = wide_df[wide_df.index.isin(missing)] \n",
    "no_premium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## THE MOST EXPENSIVE CUSTOMERS:\n",
    "\n",
    "a = paid_df.PremiumVPaid.sort_values()[0:5].index\n",
    "paid_df[paid_df.index.isin(a)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Step 5: Visualize the data to help determine appropriate modeling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt_format()\n",
    "\n",
    "plt.plot(np.sum(wide_df, axis=0).ClaimedAmount, label='Claimed Amount')\n",
    "plt.plot(np.sum(wide_df, axis=0).PaidAmount, label='Paid Amount')\n",
    "plt.title('Total Claimed & Paid Amounts in 2016, by Month')\n",
    "plt.xlim(0.9, 12.1)\n",
    "plt.xlabel('Month')\n",
    "plt.xticks([1,2,3,4,5,6,7,8,9,10,11,12], ['January', 'February', \n",
    "            'March', 'April', 'May', 'June', 'July', 'August', \n",
    "            'September', 'October', 'November', 'December'], rotation=45)\n",
    "plt.ylabel('Total Dollars')\n",
    "plt.legend(loc=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt_format()\n",
    "\n",
    "sns.distplot(paid_df.PremiumVPaid.astype(int))\n",
    "plt.title('Distribution of Premiums Collected Minus Paid Claims')\n",
    "plt.xlabel('Premium Collected Minus Paid Claims');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## DATA DISTRIBUTION COMPARISONS:\n",
    "\n",
    "plt_format()\n",
    "\n",
    "sns.violinplot(x='Churned', y='MonthlyPremium ', data=write_out, \n",
    "               split=True, inner='quartile', saturation=0.6, \n",
    "               palette={1: \"deepskyblue\", 0: \"mediumpurple\"})\n",
    "plt.title('Monthly Premium Distributions, Separated by Churn Status')\n",
    "plt.xlabel('Churned')\n",
    "plt.ylabel('Monthly Premium, USD')\n",
    "\n",
    "plt.ylim(0, 180);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## DATA DISTRIBUTION COMPARISONS:\n",
    "\n",
    "plt_format()\n",
    "\n",
    "sns.violinplot(x='Churned', y='PolicyLength', \n",
    "               data=write_out, split=True, inner='quartile', saturation=0.6, \n",
    "               palette={1: \"deepskyblue\", 0: \"mediumpurple\"})\n",
    "plt.title('Policy Length Distributions, Separated by Churn Status')\n",
    "plt.xlabel('Churned')\n",
    "plt.ylabel('Policy Length, in Months')\n",
    "\n",
    "plt.ylim(-10, 210);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## DATA DISTRIBUTION COMPARISONS:\n",
    "\n",
    "plt_format()\n",
    "\n",
    "sns.violinplot(x='Churned', y='TotalPaid', \n",
    "               data=write_out, split=True, inner='quartile', saturation=0.6, \n",
    "               palette={1: \"deepskyblue\", 0: \"mediumpurple\"})\n",
    "plt.title('Total Paid Distributions, Separated by Churn Status')\n",
    "plt.xlabel('Churned')\n",
    "plt.ylabel('Total Paid, USD');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt_format()\n",
    "\n",
    "plt.plot(paid_df.PolicyLength, paid_df.MonthlyPremium, '.')\n",
    "plt.title('Policy Length vs. Monthly Premium')\n",
    "plt.xlabel('Policy Length, in Months')\n",
    "plt.ylabel('Monthly Premium, in Dollars');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt_format()\n",
    "\n",
    "plt.plot(paid_df.EnrollDate, paid_df.MeanPaid, '.')\n",
    "plt.title('Enrollment Date vs. Average Paid Claim Amount')\n",
    "plt.xlabel('Enrollment Date')\n",
    "plt.ylabel('Mean Paid Claim Amount, in Dollars');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt_format()\n",
    "\n",
    "sns.distplot(paid_df.PolicyLength.astype(int))\n",
    "plt.title('Policy Length Distribution')\n",
    "plt.xlabel('Policy Length, in Months');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "  \n",
    "---  \n",
    "  \n",
    "---  \n",
    "  \n",
    "  \n",
    "## Model: Predicting Cancellation Probabilities \n",
    "  \n",
    " - First, run through a trial model to get an idea of how model will perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## DEFINE MODEL INPUT FEATURES & OUTPUT:\n",
    "\n",
    "df = cancel_df.copy(deep=True)\n",
    "\n",
    "input_features = ['ClaimedAmount', 'MonthlyPremium', 'PaidAmount', \n",
    "                  'MeanClaims', 'MeanPaid', 'MedianClaims', \n",
    "                  'MedianPaid', 'TotalClaims', 'TotalPaid', \n",
    "                  'TotalDifference', 'Premiums2016', 'PremiumsPaid2016', \n",
    "                  'PremiumVPaid', 'PolicyLength']\n",
    "output_feature = 'Churned'\n",
    "\n",
    "X = df[input_features]\n",
    "y = df[output_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt_format()\n",
    "\n",
    "sns.heatmap(pd.concat([pd.DataFrame(X), y], axis=1).corr())\n",
    "plt.title('Heatmap of Data Feature Correlations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## RANDOM FOREST MODELING:\n",
    "\n",
    "# Split into training & test sets:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    \n",
    "# Scale the data to a mean of '0' and standard deviation of '1'\n",
    "# Scaling the test data on the training set:\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "      \n",
    "# Initialize a classifier:\n",
    "clf = RF(n_estimators=10, criterion='entropy')\n",
    "clf.fit(X_train, y_train)\n",
    "# Predict classes:\n",
    "y_pred = clf.predict(X_test)\n",
    "# Predict probabilities:\n",
    "y_prob = clf.predict_proba(X_test)\n",
    "    \n",
    "# Print the accuracy:\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(\"  Accuracy: \", accuracy*100, '%\\n')\n",
    "\n",
    "print('  Model Statistics:')\n",
    "confusion_matrices = ConfusionMatrix(y_test, y_pred)\n",
    "confusion_matrices.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt_format()\n",
    "\n",
    "confusion_matrices.plot(backend='seaborn', annot=True, fmt=\".0f\")\n",
    "plt.title('Confusion Matrix of Cancellation Predictions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## EVALUATE PROBABILITY PREDICTIONS:\n",
    "\n",
    "# Number of times a predicted probability is assigned to an observation:\n",
    "counts = pd.value_counts(y_prob[:, 1])\n",
    "is_churn = y_test == 1\n",
    "\n",
    "# Calculate true probabilities:\n",
    "true_prob = {}\n",
    "for prob in counts.index:\n",
    "    true_prob[prob] = np.mean(is_churn[y_prob[:, 1] == prob])\n",
    "    true_prob = pd.Series(true_prob)\n",
    "\n",
    "# Reshape & rename:\n",
    "counts = pd.concat([counts, true_prob], axis=1).reset_index()\n",
    "counts.columns = ['pred_prob', 'count', 'true_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt_format()\n",
    "\n",
    "baseline = np.mean(is_churn)\n",
    "\n",
    "plt.plot(np.linspace(0, 1, 10), np.linspace(0, 1, 10), \n",
    "         c=\"#95a5a6\", linewidth=3, alpha=0.8, label='Ideal Predection Line')\n",
    "plt.plot(np.linspace(0, 1, 10), np.linspace(baseline, baseline, 10), \n",
    "         c=\"#3498db\", linewidth=3, alpha=0.8, label='Mean Probability of Churn')\n",
    "plt.scatter(data=counts, x='pred_prob', y='true_prob', s='count', \n",
    "            marker='o', label=None, alpha=0.7, c=\"#9b59b6\")\n",
    "plt.title(\"Random Forest Model Outcomes\")\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.ylabel(\"Relative Frequency of Outcome\")\n",
    "plt.xlim(-0.05,  1.05)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.legend(loc=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt_format()\n",
    "\n",
    "conf_mat = pd.DataFrame(\n",
    "    confusion_matrix(y_test.values, y_pred), \n",
    "    columns=[\"Predicted False\", \"Predicted True\"], \n",
    "    index=[\"Actual False\", \"Actual True\"]\n",
    ")\n",
    "display(conf_mat)\n",
    "\n",
    "# Calculate the false positives & true positives for all thresholds of the classification\n",
    "fpr, tpr, threshold = roc_curve(y_test, y_prob[:, 1])\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, color='deepskyblue', linewidth=3)\n",
    "plt.plot([0, 1], [0, 1], '--', color='mediumpurple', linewidth=3)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 18))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "df_f = pd.DataFrame(clf.feature_importances_, columns=[\"Importance\"])\n",
    "df_f[\"Labels\"] = df[input_features].columns\n",
    "df_f.sort_values(\"Importance\", inplace=True, ascending=False)\n",
    "display(df_f.head(5))\n",
    "\n",
    "index = np.arange(len(clf.feature_importances_))\n",
    "bar_width = 0.7\n",
    "rects = plt.barh(index , df_f[\"Importance\"], bar_width, alpha=0.4, color='b', label='Main')\n",
    "plt.yticks(index, df_f[\"Labels\"])\n",
    "plt.title('Proportion of Importance for each Model Input Feature')\n",
    "plt.xlabel('Importance, Proportion')\n",
    "plt.ylabel('Model Input Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## PREVIEW CHURN PROBABILITIES:\n",
    "\n",
    "pd.DataFrame(y_prob).iloc[:, 1].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "  \n",
    "- Final cancellation prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## RANDOM FOREST MODELING; TRAIN FINAL MODEL WITH OUTPUT:\n",
    "\n",
    "# Scale the data to a mean of '0' and standard deviation of '1':\n",
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "      \n",
    "# Initialize a classifier:\n",
    "clf = RF(n_estimators=4, criterion='entropy', random_state=None)\n",
    "clf.fit(X, y)\n",
    "# Predict classes:\n",
    "y_pred = clf.predict(X)\n",
    "# Predict probabilities:\n",
    "y_prob = clf.predict_proba(X)\n",
    "    \n",
    "# Print the accuracy:\n",
    "accuracy = clf.score(X, y)\n",
    "print(\"  Accuracy: \", accuracy*100, '%\\n')\n",
    "\n",
    "print('  Model Statistics:')\n",
    "confusion_matrices = ConfusionMatrix(y, y_pred)\n",
    "confusion_matrices.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## EVALUATE PROBABILITY PREDICTIONS:\n",
    "\n",
    "# Number of times a predicted probability is assigned to an observation:\n",
    "counts = pd.value_counts(y_prob[:, 1])\n",
    "is_churn = y == 1\n",
    "\n",
    "# Calculate true probabilities:\n",
    "true_prob = {}\n",
    "for prob in counts.index:\n",
    "    true_prob[prob] = np.mean(is_churn[y_prob[:, 1] == prob])\n",
    "    true_prob = pd.Series(true_prob)\n",
    "\n",
    "# Reshape & rename:\n",
    "counts = pd.concat([counts, true_prob], axis=1).reset_index()\n",
    "counts.columns = ['pred_prob', 'count', 'true_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## PLOT PREDICTION COUNTS:\n",
    "\n",
    "plt_format()\n",
    "\n",
    "baseline = np.mean(is_churn)\n",
    "\n",
    "plt.plot(np.linspace(0, 1, 10), np.linspace(0, 1, 10), \n",
    "         c=\"#95a5a6\", linewidth=3, alpha=0.8, label='Ideal Predection Line')\n",
    "plt.plot(np.linspace(0, 1, 10), np.linspace(baseline, baseline, 10), \n",
    "         c=\"#3498db\", linewidth=3, alpha=0.8, label='Mean Probability of Churn')\n",
    "plt.scatter(data=counts, x='pred_prob', y='true_prob', s='count', \n",
    "            marker='o', label=None, alpha=0.7, c=\"#9b59b6\")\n",
    "plt.title(\"Random Forest Model Outcomes\")\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.ylabel(\"Relative Frequency of Outcome\")\n",
    "plt.xlim(-0.05,  1.05)\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.legend(loc=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## WRITE OUT CANCELLATION PROBABILITIES:\n",
    "\n",
    "cancel_probs = pd.DataFrame(y_prob).iloc[:, 1]\n",
    "cancel_probs = pd.concat([df.PolicyId, cancel_probs], axis=1)\n",
    "cancel_probs.columns = ['PolicyId', 'CancelProb']\n",
    "cancel_probs.to_csv('./Data/CancellationProbabilities.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
